{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "#intro\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Exemple RDD\")\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd_squared = rdd.map(lambda x: x ** 2)\n",
    "print(rdd_squared.collect())  # Résultat : [1, 4, 9, 16, 25]\n",
    "\n",
    "# Arrêter proprement SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres pairs : [2, 4, 6, 8, 10]\n",
      "Carrés des nombres pairs : [4, 16, 36, 64, 100]\n",
      "Somme des carrés : 220\n"
     ]
    }
   ],
   "source": [
    "#ex1\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Création du SparkContext\n",
    "sc = SparkContext(\"local\", \"Exercice 1\")\n",
    "\n",
    "# Création d'un RDD à partir d'une liste de nombres\n",
    "numbers = range(1, 11)  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(numbers)\n",
    "\n",
    "# Filtrer les nombres pairs\n",
    "even_numbers = rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Calculer le carré de chaque nombre filtré\n",
    "squared_numbers = even_numbers.map(lambda x: x ** 2)\n",
    "\n",
    "# Calculer la somme des carrés\n",
    "sum_squared = squared_numbers.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Nombres pairs :\", even_numbers.collect())\n",
    "print(\"Carrés des nombres pairs :\", squared_numbers.collect())\n",
    "print(\"Somme des carrés :\", sum_squared)\n",
    "\n",
    "# Arrêter le SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schéma du DataFrame :\n",
      "root\n",
      " |-- nom: string (nullable = true)\n",
      " |-- departement: string (nullable = true)\n",
      " |-- salaire: long (nullable = true)\n",
      "\n",
      "\n",
      "Employés avec un salaire > 50000 :\n",
      "+-------+-----------+-------+\n",
      "|    nom|departement|salaire|\n",
      "+-------+-----------+-------+\n",
      "|  Alice|         IT|  60000|\n",
      "|Charlie|         IT|  75000|\n",
      "|    Eve|         IT|  55000|\n",
      "+-------+-----------+-------+\n",
      "\n",
      "\n",
      "Salaire moyen par département :\n",
      "+-----------+------------------+\n",
      "|departement|      avg(salaire)|\n",
      "+-----------+------------------+\n",
      "|         IT|63333.333333333336|\n",
      "|         HR|           45000.0|\n",
      "|  Marketing|           48000.0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création de la SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exercice 2\").getOrCreate()\n",
    "\n",
    "# Création des données\n",
    "employee_data = [\n",
    "    (\"Alice\", \"IT\", 60000),\n",
    "    (\"Bob\", \"HR\", 45000),\n",
    "    (\"Charlie\", \"IT\", 75000),\n",
    "    (\"David\", \"Marketing\", 48000),\n",
    "    (\"Eve\", \"IT\", 55000)\n",
    "]\n",
    "\n",
    "# Création du DataFrame\n",
    "df = spark.createDataFrame(employee_data, [\"nom\", \"departement\", \"salaire\"])\n",
    "\n",
    "# 1. Afficher le schéma du DataFrame\n",
    "print(\"Schéma du DataFrame :\")\n",
    "df.printSchema()\n",
    "\n",
    "# 2. Filtrer les employés avec un salaire > 50000\n",
    "print(\"\\nEmployés avec un salaire > 50000 :\")\n",
    "df.filter(df.salaire > 50000).show()\n",
    "\n",
    "# 3. Calculer le salaire moyen par département\n",
    "print(\"\\nSalaire moyen par département :\")\n",
    "df.groupBy(\"departement\").avg(\"salaire\").show()\n",
    "\n",
    "# Arrêter la SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 premiers produits :\n",
      "+---+----------+-----------+------+\n",
      "| id|       nom|  categorie|  prix|\n",
      "+---+----------+-----------+------+\n",
      "|  1|    Laptop|Electronics|999.99|\n",
      "|  2|Smartphone|Electronics|699.99|\n",
      "|  3|   T-shirt|   Clothing| 19.99|\n",
      "|  4|     Jeans|   Clothing| 49.99|\n",
      "|  5|Headphones|Electronics|149.99|\n",
      "+---+----------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Produits Electronics :\n",
      "+---+----------+-----------+------+\n",
      "| id|       nom|  categorie|  prix|\n",
      "+---+----------+-----------+------+\n",
      "|  1|    Laptop|Electronics|999.99|\n",
      "|  2|Smartphone|Electronics|699.99|\n",
      "|  5|Headphones|Electronics|149.99|\n",
      "|  7|    Tablet|Electronics|299.99|\n",
      "+---+----------+-----------+------+\n",
      "\n",
      "\n",
      "Prix moyen par catégorie :\n",
      "+-----------+---------+\n",
      "|  categorie|avg(prix)|\n",
      "+-----------+---------+\n",
      "|Electronics|   537.49|\n",
      "|   Clothing|    34.99|\n",
      "|      Books|    14.99|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex3\n",
    "# \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création de la SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exercice 3\").getOrCreate()\n",
    "\n",
    "# Création des données\n",
    "products_data = [\n",
    "    (1, \"Laptop\", \"Electronics\", 999.99),\n",
    "    (2, \"Smartphone\", \"Electronics\", 699.99),\n",
    "    (3, \"T-shirt\", \"Clothing\", 19.99),\n",
    "    (4, \"Jeans\", \"Clothing\", 49.99),\n",
    "    (5, \"Headphones\", \"Electronics\", 149.99),\n",
    "    (6, \"Book\", \"Books\", 14.99),\n",
    "    (7, \"Tablet\", \"Electronics\", 299.99)\n",
    "]\n",
    "\n",
    "# Création du DataFrame\n",
    "df = spark.createDataFrame(products_data, [\"id\", \"nom\", \"categorie\", \"prix\"])\n",
    "\n",
    "# Au lieu d'écrire en CSV, travaillons directement avec le DataFrame\n",
    "print(\"Les 5 premiers produits :\")\n",
    "df.show(5)\n",
    "\n",
    "print(\"\\nProduits Electronics :\")\n",
    "df.filter(df.categorie == \"Electronics\").show()\n",
    "\n",
    "print(\"\\nPrix moyen par catégorie :\")\n",
    "df.groupBy(\"categorie\").avg(\"prix\").show()\n",
    "\n",
    "# Arrêter la SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde du fichier CSV...\n",
      "\n",
      "Lecture du fichier CSV...\n",
      "\n",
      "Les 5 premiers produits :\n",
      "+---+----------+-----------+------+\n",
      "| id|       nom|  categorie|  prix|\n",
      "+---+----------+-----------+------+\n",
      "|  2|Smartphone|Electronics|699.99|\n",
      "|  5|Headphones|Electronics|149.99|\n",
      "|  1|    Laptop|Electronics|999.99|\n",
      "|  7|    Tablet|Electronics|299.99|\n",
      "|  3|   T-shirt|   Clothing| 19.99|\n",
      "+---+----------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Produits Electronics :\n",
      "+---+----------+-----------+------+\n",
      "| id|       nom|  categorie|  prix|\n",
      "+---+----------+-----------+------+\n",
      "|  2|Smartphone|Electronics|699.99|\n",
      "|  5|Headphones|Electronics|149.99|\n",
      "|  1|    Laptop|Electronics|999.99|\n",
      "|  7|    Tablet|Electronics|299.99|\n",
      "+---+----------+-----------+------+\n",
      "\n",
      "\n",
      "Prix moyen par catégorie :\n",
      "+-----------+---------+\n",
      "|  categorie|avg(prix)|\n",
      "+-----------+---------+\n",
      "|Electronics|   537.49|\n",
      "|   Clothing|    34.99|\n",
      "|      Books|    14.99|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex3 après hadoop\n",
    "# \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création de la SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exercice 3\").getOrCreate()\n",
    "\n",
    "# Création des données\n",
    "products_data = [\n",
    "    (1, \"Laptop\", \"Electronics\", 999.99),\n",
    "    (2, \"Smartphone\", \"Electronics\", 699.99),\n",
    "    (3, \"T-shirt\", \"Clothing\", 19.99),\n",
    "    (4, \"Jeans\", \"Clothing\", 49.99),\n",
    "    (5, \"Headphones\", \"Electronics\", 149.99),\n",
    "    (6, \"Book\", \"Books\", 14.99),\n",
    "    (7, \"Tablet\", \"Electronics\", 299.99)\n",
    "]\n",
    "\n",
    "# Création du DataFrame\n",
    "df = spark.createDataFrame(products_data, [\"id\", \"nom\", \"categorie\", \"prix\"])\n",
    "\n",
    "# 1. Sauvegarde en CSV avec chemin complet et mode overwrite\n",
    "print(\"Sauvegarde du fichier CSV...\")\n",
    "df.write.mode(\"overwrite\").csv(\"C:/EXERCICES/pyspark/PySpark/products.csv\", header=True)\n",
    "\n",
    "# 2. Lecture du fichier CSV\n",
    "print(\"\\nLecture du fichier CSV...\")\n",
    "products_df = spark.read.csv(\"C:/EXERCICES/pyspark/PySpark/products.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 3. Afficher les 5 premières lignes\n",
    "print(\"\\nLes 5 premiers produits :\")\n",
    "products_df.show(5)\n",
    "\n",
    "# 4. Filtrer les produits Electronics\n",
    "print(\"\\nProduits Electronics :\")\n",
    "products_df.filter(products_df.categorie == \"Electronics\").show()\n",
    "\n",
    "# 5. Calculer le prix moyen par catégorie\n",
    "print(\"\\nPrix moyen par catégorie :\")\n",
    "products_df.groupBy(\"categorie\").avg(\"prix\").show()\n",
    "\n",
    "# Arrêter la SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produits avec classification de prix :\n",
      "+---+----------+-----------+------+----------+\n",
      "| id|       nom|  categorie|  prix|gamme_prix|\n",
      "+---+----------+-----------+------+----------+\n",
      "|  2|Smartphone|Electronics|699.99|   Premium|\n",
      "|  5|Headphones|Electronics|149.99|  Standard|\n",
      "|  1|    Laptop|Electronics|999.99|   Premium|\n",
      "|  7|    Tablet|Electronics|299.99|  Standard|\n",
      "|  3|   T-shirt|   Clothing| 19.99|    Budget|\n",
      "|  4|     Jeans|   Clothing| 49.99|    Budget|\n",
      "|  6|      Book|      Books| 14.99|    Budget|\n",
      "+---+----------+-----------+------+----------+\n",
      "\n",
      "\n",
      "Nombre de produits par gamme de prix :\n",
      "+----------+-----+\n",
      "|gamme_prix|count|\n",
      "+----------+-----+\n",
      "|   Premium|    2|\n",
      "|  Standard|    2|\n",
      "|    Budget|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex4\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Création de la SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exercice 4\").getOrCreate()\n",
    "\n",
    "# Utiliser le DataFrame de produits de l'exercice précédent\n",
    "products_df = spark.read.csv(\"C:/EXERCICES/pyspark/PySpark/products.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Définir une fonction pour classifier les prix\n",
    "def classify_price(price):\n",
    "    if price >= 500:\n",
    "        return \"Premium\"\n",
    "    elif price >= 100:\n",
    "        return \"Standard\"\n",
    "    else:\n",
    "        return \"Budget\"\n",
    "\n",
    "# Créer une UDF (User Defined Function)\n",
    "price_classifier_udf = udf(classify_price, StringType())\n",
    "\n",
    "# Appliquer l'UDF pour ajouter une nouvelle colonne\n",
    "products_with_category = products_df.withColumn(\n",
    "    \"gamme_prix\", \n",
    "    price_classifier_udf(products_df.prix)\n",
    ")\n",
    "\n",
    "# Afficher le résultat\n",
    "print(\"Produits avec classification de prix :\")\n",
    "products_with_category.show()\n",
    "\n",
    "# Analyser la distribution des produits par gamme de prix\n",
    "print(\"\\nNombre de produits par gamme de prix :\")\n",
    "products_with_category.groupBy(\"gamme_prix\").count().show()\n",
    "\n",
    "# Arrêter la SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commandes avec informations clients :\n",
      "+--------+-----------+------+----------+-----------+-------------+-------+\n",
      "|order_id|customer_id|amount|      date|customer_id|         name|country|\n",
      "+--------+-----------+------+----------+-----------+-------------+-------+\n",
      "|       1|        101|999.99|2024-01-15|        101| Alice Martin| France|\n",
      "|       2|        102|149.99|2024-01-16|        102|    Bob Smith|    USA|\n",
      "|       3|        101| 49.99|2024-01-17|        101| Alice Martin| France|\n",
      "|       4|        103|699.99|2024-01-18|        103|Charlie Brown|     UK|\n",
      "|       5|        102|299.99|2024-01-19|        102|    Bob Smith|    USA|\n",
      "+--------+-----------+------+----------+-----------+-------------+-------+\n",
      "\n",
      "\n",
      "Montant total des commandes par pays :\n",
      "+-------+-----------+\n",
      "|country|sum(amount)|\n",
      "+-------+-----------+\n",
      "| France|    1049.98|\n",
      "|     UK|     699.99|\n",
      "|    USA|     449.98|\n",
      "+-------+-----------+\n",
      "\n",
      "\n",
      "Top clients par montant total des commandes :\n",
      "+-------------+-------+-----------+\n",
      "|         name|country|sum(amount)|\n",
      "+-------------+-------+-----------+\n",
      "| Alice Martin| France|    1049.98|\n",
      "|Charlie Brown|     UK|     699.99|\n",
      "|    Bob Smith|    USA|     449.98|\n",
      "+-------------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex5\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création de la SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exercice 5\").getOrCreate()\n",
    "\n",
    "# Création des données pour les commandes\n",
    "orders_data = [\n",
    "    (1, 101, 999.99, \"2024-01-15\"),\n",
    "    (2, 102, 149.99, \"2024-01-16\"),\n",
    "    (3, 101, 49.99, \"2024-01-17\"),\n",
    "    (4, 103, 699.99, \"2024-01-18\"),\n",
    "    (5, 102, 299.99, \"2024-01-19\")\n",
    "]\n",
    "\n",
    "# Création des données pour les clients\n",
    "customers_data = [\n",
    "    (101, \"Alice Martin\", \"France\"),\n",
    "    (102, \"Bob Smith\", \"USA\"),\n",
    "    (103, \"Charlie Brown\", \"UK\"),\n",
    "    (104, \"David Wilson\", \"Canada\")\n",
    "]\n",
    "\n",
    "# Création des DataFrames\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"amount\", \"date\"])\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\", \"country\"])\n",
    "\n",
    "# Jointure des DataFrames\n",
    "orders_with_customers = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df.customer_id == customers_df.customer_id,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Commandes avec informations clients :\")\n",
    "orders_with_customers.show()\n",
    "\n",
    "# Calculer le montant total des commandes par pays\n",
    "print(\"\\nMontant total des commandes par pays :\")\n",
    "orders_with_customers.groupBy(\"country\") \\\n",
    "    .sum(\"amount\") \\\n",
    "    .orderBy(\"country\") \\\n",
    "    .show()\n",
    "\n",
    "# Trouver le client avec le montant total le plus élevé\n",
    "print(\"\\nTop clients par montant total des commandes :\")\n",
    "orders_with_customers.groupBy(\"name\", \"country\") \\\n",
    "    .sum(\"amount\") \\\n",
    "    .orderBy(\"sum(amount)\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "# Arrêter la SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commandes avec informations clients :\n",
      "+-----------+--------+------+----------+-------------+-------+\n",
      "|customer_id|order_id|amount|      date|         name|country|\n",
      "+-----------+--------+------+----------+-------------+-------+\n",
      "|        103|       4|699.99|2024-01-18|Charlie Brown|     UK|\n",
      "|        101|       1|999.99|2024-01-15| Alice Martin| France|\n",
      "|        101|       3| 49.99|2024-01-17| Alice Martin| France|\n",
      "|        102|       2|149.99|2024-01-16|    Bob Smith|    USA|\n",
      "|        102|       5|299.99|2024-01-19|    Bob Smith|    USA|\n",
      "+-----------+--------+------+----------+-------------+-------+\n",
      "\n",
      "\n",
      "Montant total des commandes par pays :\n",
      "+-------+-----------+\n",
      "|country|sum(amount)|\n",
      "+-------+-----------+\n",
      "|     UK|     699.99|\n",
      "| France|    1049.98|\n",
      "|    USA|     449.98|\n",
      "+-------+-----------+\n",
      "\n",
      "\n",
      "Top clients par montant total :\n",
      "+-------------+-------+-----------+\n",
      "|         name|country|sum(amount)|\n",
      "+-------------+-------+-----------+\n",
      "| Alice Martin| France|    1049.98|\n",
      "|Charlie Brown|     UK|     699.99|\n",
      "|    Bob Smith|    USA|     449.98|\n",
      "+-------------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex 5 optimisé car lent \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Créer une nouvelle SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exercice 5\").getOrCreate()\n",
    "\n",
    "# Création des données\n",
    "orders_data = [\n",
    "    (1, 101, 999.99, \"2024-01-15\"),\n",
    "    (2, 102, 149.99, \"2024-01-16\"),\n",
    "    (3, 101, 49.99, \"2024-01-17\"),\n",
    "    (4, 103, 699.99, \"2024-01-18\"),\n",
    "    (5, 102, 299.99, \"2024-01-19\")\n",
    "]\n",
    "\n",
    "customers_data = [\n",
    "    (101, \"Alice Martin\", \"France\"),\n",
    "    (102, \"Bob Smith\", \"USA\"),\n",
    "    (103, \"Charlie Brown\", \"UK\"),\n",
    "    (104, \"David Wilson\", \"Canada\")\n",
    "]\n",
    "\n",
    "# Création des DataFrames avec cache()\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"amount\", \"date\"]).cache()\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\", \"country\"]).cache()\n",
    "\n",
    "# Jointure avec optimisation\n",
    "orders_with_customers = orders_df.join(\n",
    "    customers_df,\n",
    "    \"customer_id\",\n",
    "    \"left\"\n",
    ").cache()\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Commandes avec informations clients :\")\n",
    "orders_with_customers.show()\n",
    "\n",
    "print(\"\\nMontant total des commandes par pays :\")\n",
    "orders_with_customers.groupBy(\"country\").sum(\"amount\").show()\n",
    "\n",
    "print(\"\\nTop clients par montant total :\")\n",
    "orders_with_customers.groupBy(\"name\", \"country\").sum(\"amount\").orderBy(\"sum(amount)\", ascending=False).show()\n",
    "\n",
    "# Ne pas arrêter la SparkSession cette fois-ci pour pouvoir continuer avec d'autres exercices\n",
    "\n",
    "# Ne pas arrêter la SparkSession ici si vous voulez continuer avec d'autres exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montant total par pays et catégorie :\n",
      "+-------+-----------+-----------+\n",
      "|country|   category|sum(amount)|\n",
      "+-------+-----------+-----------+\n",
      "| Canada|Electronics|     899.99|\n",
      "| France|   Clothing|      29.99|\n",
      "| France|Electronics|    1299.99|\n",
      "|     UK|Electronics|     549.99|\n",
      "|    USA|      Books|      79.99|\n",
      "+-------+-----------+-----------+\n",
      "\n",
      "\n",
      "Nombre de commandes par pays :\n",
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "| France|    2|\n",
      "|    USA|    1|\n",
      "|     UK|    1|\n",
      "| Canada|    1|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "Moyenne des commandes par catégorie :\n",
      "+-----------+-----------------+\n",
      "|   category|      avg(amount)|\n",
      "+-----------+-----------------+\n",
      "|Electronics|916.6566666666668|\n",
      "|      Books|            79.99|\n",
      "|   Clothing|            29.99|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex6\n",
    "# \n",
    "# Utilisons les données des commandes de l'exercice précédent et ajoutons plus de données\n",
    "\n",
    "# Création de nouvelles commandes avec plus de données\n",
    "more_orders_data = [\n",
    "    (6, 101, 1299.99, \"2024-01-20\", \"Electronics\"),\n",
    "    (7, 102, 79.99, \"2024-01-20\", \"Books\"),\n",
    "    (8, 103, 549.99, \"2024-01-21\", \"Electronics\"),\n",
    "    (9, 101, 29.99, \"2024-01-21\", \"Clothing\"),\n",
    "    (10, 104, 899.99, \"2024-01-22\", \"Electronics\")\n",
    "]\n",
    "\n",
    "# Créer un nouveau DataFrame avec les commandes étendues\n",
    "extended_orders_df = spark.createDataFrame(\n",
    "    more_orders_data, \n",
    "    [\"order_id\", \"customer_id\", \"amount\", \"date\", \"category\"]\n",
    ").cache()\n",
    "\n",
    "# Joindre avec les informations clients\n",
    "orders_complete = extended_orders_df.join(\n",
    "    customers_df,\n",
    "    \"customer_id\",\n",
    "    \"left\"\n",
    ").cache()\n",
    "\n",
    "# 1. Montant total des commandes par pays et catégorie\n",
    "print(\"Montant total par pays et catégorie :\")\n",
    "orders_complete.groupBy(\"country\", \"category\") \\\n",
    "    .agg({\"amount\": \"sum\"}) \\\n",
    "    .orderBy(\"country\", \"category\") \\\n",
    "    .show()\n",
    "\n",
    "# 2. Nombre de commandes par pays\n",
    "print(\"\\nNombre de commandes par pays :\")\n",
    "orders_complete.groupBy(\"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "# 3. Moyenne des commandes par catégorie\n",
    "print(\"\\nMoyenne des commandes par catégorie :\")\n",
    "orders_complete.groupBy(\"category\") \\\n",
    "    .agg({\"amount\": \"avg\"}) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame original avec valeurs manquantes :\n",
      "+---+----------+-----------+------+------+\n",
      "| id|   product|   category| price|rating|\n",
      "+---+----------+-----------+------+------+\n",
      "|  1|    Laptop|Electronics|999.99|   4.5|\n",
      "|  2|Smartphone|       NULL|699.99|  NULL|\n",
      "|  3|   T-shirt|   Clothing|  NULL|   4.0|\n",
      "|  4|     Jeans|   Clothing| 49.99|   4.2|\n",
      "|  5|      NULL|Electronics|149.99|   3.8|\n",
      "|  6|      Book|       NULL| 14.99|  NULL|\n",
      "+---+----------+-----------+------+------+\n",
      "\n",
      "\n",
      "Nombre de valeurs manquantes par colonne :\n",
      "id: 0 valeurs manquantes\n",
      "product: 1 valeurs manquantes\n",
      "category: 2 valeurs manquantes\n",
      "price: 1 valeurs manquantes\n",
      "rating: 2 valeurs manquantes\n",
      "\n",
      "DataFrame après traitement des valeurs manquantes :\n",
      "+---+----------+-----------+------+------+\n",
      "| id|   product|   category| price|rating|\n",
      "+---+----------+-----------+------+------+\n",
      "|  1|    Laptop|Electronics|999.99|   4.5|\n",
      "|  2|Smartphone|      Other|699.99|   0.0|\n",
      "|  3|   T-shirt|   Clothing|382.99|   4.0|\n",
      "|  4|     Jeans|   Clothing| 49.99|   4.2|\n",
      "|  5|   Unknown|Electronics|149.99|   3.8|\n",
      "|  6|      Book|      Other| 14.99|   0.0|\n",
      "+---+----------+-----------+------+------+\n",
      "\n",
      "\n",
      "Statistiques par catégorie après nettoyage :\n",
      "+-----------+--------------+----------+-----------+\n",
      "|   category|count(product)|avg(price)|avg(rating)|\n",
      "+-----------+--------------+----------+-----------+\n",
      "|Electronics|             2|    574.99|       4.15|\n",
      "|      Other|             2|    357.49|        0.0|\n",
      "|   Clothing|             2|    216.49|        4.1|\n",
      "+-----------+--------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex7\n",
    "\n",
    "# Créer un DataFrame avec des valeurs manquantes\n",
    "data_with_nulls = [\n",
    "    (1, \"Laptop\", \"Electronics\", 999.99, 4.5),\n",
    "    (2, \"Smartphone\", None, 699.99, None),\n",
    "    (3, \"T-shirt\", \"Clothing\", None, 4.0),\n",
    "    (4, \"Jeans\", \"Clothing\", 49.99, 4.2),\n",
    "    (5, None, \"Electronics\", 149.99, 3.8),\n",
    "    (6, \"Book\", None, 14.99, None)\n",
    "]\n",
    "\n",
    "# Créer le DataFrame\n",
    "df_nulls = spark.createDataFrame(\n",
    "    data_with_nulls, \n",
    "    [\"id\", \"product\", \"category\", \"price\", \"rating\"]\n",
    ").cache()\n",
    "\n",
    "print(\"DataFrame original avec valeurs manquantes :\")\n",
    "df_nulls.show()\n",
    "\n",
    "# 1. Compter les valeurs manquantes par colonne\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "print(\"\\nNombre de valeurs manquantes par colonne :\")\n",
    "for column in df_nulls.columns:\n",
    "    null_count = df_nulls.filter(\n",
    "        col(column).isNull() | isnan(col(column))\n",
    "    ).count()\n",
    "    print(f\"{column}: {null_count} valeurs manquantes\")\n",
    "\n",
    "# 2. Remplacer les valeurs manquantes\n",
    "from pyspark.sql.functions import avg, coalesce, lit\n",
    "\n",
    "# Calculer le prix moyen\n",
    "avg_price = df_nulls.select(avg(\"price\")).collect()[0][0]\n",
    "\n",
    "# Remplacer les valeurs manquantes\n",
    "df_cleaned = df_nulls \\\n",
    "    .fillna(\"Unknown\", subset=[\"product\"]) \\\n",
    "    .fillna(\"Other\", subset=[\"category\"]) \\\n",
    "    .fillna(avg_price, subset=[\"price\"]) \\\n",
    "    .fillna(0.0, subset=[\"rating\"])\n",
    "\n",
    "print(\"\\nDataFrame après traitement des valeurs manquantes :\")\n",
    "df_cleaned.show()\n",
    "\n",
    "# 3. Statistiques après nettoyage\n",
    "print(\"\\nStatistiques par catégorie après nettoyage :\")\n",
    "df_cleaned.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        {\"price\": \"avg\", \"rating\": \"avg\", \"product\": \"count\"}\n",
    "    ) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de partitions par défaut : 20\n",
      "Nombre de partitions après repartition : 4\n",
      "\n",
      "Test de performance avec différentes partitions:\n",
      "DataFrame original: 20.03 secondes\n",
      "DataFrame repartitionné: 20.29 secondes\n",
      "\n",
      "Exemple de données par partition (prix):\n",
      "+---+----------+-----+\n",
      "| id|      name|price|\n",
      "+---+----------+-----+\n",
      "|  0| product_0|  0.0|\n",
      "|  5| product_5|  0.0|\n",
      "| 10|product_10|  0.0|\n",
      "| 15|product_15|  0.0|\n",
      "| 20|product_20|  0.0|\n",
      "+---+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex8\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Créer un plus grand jeu de données pour démontrer le partitionnement\n",
    "large_data = [(i, f\"product_{i}\", i % 5 * 100.0) for i in range(1000)]\n",
    "\n",
    "# Créer le DataFrame\n",
    "large_df = spark.createDataFrame(large_data, [\"id\", \"name\", \"price\"])\n",
    "\n",
    "# Vérifier le nombre de partitions par défaut\n",
    "print(\"Nombre de partitions par défaut :\", large_df.rdd.getNumPartitions())\n",
    "\n",
    "# Repartitionner le DataFrame\n",
    "df_repartitioned = large_df.repartition(4)\n",
    "print(\"Nombre de partitions après repartition :\", df_repartitioned.rdd.getNumPartitions())\n",
    "\n",
    "# Comparer les performances\n",
    "def show_execution_time(df, description):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    # Forcer l'exécution avec count()\n",
    "    count = df.count()\n",
    "    end_time = time.time()\n",
    "    print(f\"{description}: {end_time - start_time:.2f} secondes\")\n",
    "\n",
    "# Test avec différentes configurations\n",
    "print(\"\\nTest de performance avec différentes partitions:\")\n",
    "show_execution_time(large_df, \"DataFrame original\")\n",
    "show_execution_time(df_repartitioned, \"DataFrame repartitionné\")\n",
    "\n",
    "# Démonstration de partitionnement par colonne\n",
    "df_partitioned = large_df.repartition(\"price\")\n",
    "print(\"\\nExemple de données par partition (prix):\")\n",
    "df_partitioned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des produits avec leur prix:\n",
      "+----------+------+\n",
      "|      name| price|\n",
      "+----------+------+\n",
      "|    Laptop|999.99|\n",
      "|Smartphone|699.99|\n",
      "|Headphones|149.99|\n",
      "|     Jeans| 49.99|\n",
      "|   T-shirt| 19.99|\n",
      "+----------+------+\n",
      "\n",
      "\n",
      "Total des ventes par catégorie:\n",
      "+-----------+------------------+---------------+\n",
      "|   category|       total_sales|number_of_sales|\n",
      "+-----------+------------------+---------------+\n",
      "|Electronics|5099.9400000000005|              4|\n",
      "|   Clothing|             59.97|              1|\n",
      "+-----------+------------------+---------------+\n",
      "\n",
      "\n",
      "Produits les plus vendus:\n",
      "+----------+-----------+--------------+------------------+------------+\n",
      "|      name|   category|total_quantity|     total_revenue|revenue_rank|\n",
      "+----------+-----------+--------------+------------------+------------+\n",
      "|    Laptop|Electronics|             3|2999.9700000000003|           1|\n",
      "|Smartphone|Electronics|             3|2099.9700000000003|           2|\n",
      "|   T-shirt|   Clothing|             3|             59.97|           3|\n",
      "+----------+-----------+--------------+------------------+------------+\n",
      "\n",
      "\n",
      "Statistiques par catégorie:\n",
      "+-----------+-------------+---------+---------+---------+\n",
      "|   category|product_count|avg_price|min_price|max_price|\n",
      "+-----------+-------------+---------+---------+---------+\n",
      "|Electronics|            3|   616.66|   149.99|   999.99|\n",
      "|   Clothing|            2|    34.99|    19.99|    49.99|\n",
      "+-----------+-------------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex9\n",
    "\n",
    "# Créer des données pour les produits et les ventes\n",
    "products_data = [\n",
    "    (1, \"Laptop\", \"Electronics\", 999.99),\n",
    "    (2, \"Smartphone\", \"Electronics\", 699.99),\n",
    "    (3, \"T-shirt\", \"Clothing\", 19.99),\n",
    "    (4, \"Jeans\", \"Clothing\", 49.99),\n",
    "    (5, \"Headphones\", \"Electronics\", 149.99)\n",
    "]\n",
    "\n",
    "sales_data = [\n",
    "    (1, 1, \"2024-01-01\", 2),\n",
    "    (2, 2, \"2024-01-01\", 1),\n",
    "    (3, 1, \"2024-01-02\", 1),\n",
    "    (4, 3, \"2024-01-02\", 3),\n",
    "    (5, 2, \"2024-01-03\", 2)\n",
    "]\n",
    "\n",
    "# Créer les DataFrames\n",
    "products_df = spark.createDataFrame(products_data, [\"product_id\", \"name\", \"category\", \"price\"])\n",
    "sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"product_id\", \"date\", \"quantity\"])\n",
    "\n",
    "# Créer des vues temporaires pour utiliser SQL\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# 1. Requête simple\n",
    "print(\"Liste des produits avec leur prix:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT name, price\n",
    "    FROM products\n",
    "    ORDER BY price DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 2. Jointure avec agrégation\n",
    "print(\"\\nTotal des ventes par catégorie:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        SUM(p.price * s.quantity) as total_sales,\n",
    "        COUNT(s.sale_id) as number_of_sales\n",
    "    FROM products p\n",
    "    JOIN sales s ON p.product_id = s.product_id\n",
    "    GROUP BY p.category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 3. Analyse plus complexe\n",
    "print(\"\\nProduits les plus vendus:\")\n",
    "spark.sql(\"\"\"\n",
    "    WITH product_sales AS (\n",
    "        SELECT \n",
    "            p.name,\n",
    "            p.category,\n",
    "            SUM(s.quantity) as total_quantity,\n",
    "            SUM(p.price * s.quantity) as total_revenue\n",
    "        FROM products p\n",
    "        JOIN sales s ON p.product_id = s.product_id\n",
    "        GROUP BY p.name, p.category\n",
    "    )\n",
    "    SELECT \n",
    "        name,\n",
    "        category,\n",
    "        total_quantity,\n",
    "        total_revenue,\n",
    "        RANK() OVER (ORDER BY total_revenue DESC) as revenue_rank\n",
    "    FROM product_sales\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 4. Statistiques par catégorie\n",
    "print(\"\\nStatistiques par catégorie:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as product_count,\n",
    "        ROUND(AVG(price), 2) as avg_price,\n",
    "        MIN(price) as min_price,\n",
    "        MAX(price) as max_price\n",
    "    FROM products\n",
    "    GROUP BY category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classement des salaires par département:\n",
      "+---+-------+----------+------+----------+----+----------+----------+\n",
      "| id|   name|department|salary| hire_date|rank|dense_rank|row_number|\n",
      "+---+-------+----------+------+----------+----+----------+----------+\n",
      "|  7|  Grace|   Finance| 80000|2021-12-01|   1|         1|         1|\n",
      "|  6|  Frank|   Finance| 70000|2023-01-01|   2|         2|         2|\n",
      "|  5|    Eve|        HR| 60000|2022-07-01|   1|         1|         1|\n",
      "|  3|Charlie|        HR| 55000|2023-03-01|   2|         2|         2|\n",
      "|  8|  Henry|        IT| 90000|2022-09-15|   1|         1|         1|\n",
      "|  4|  David|        IT| 85000|2021-01-01|   2|         2|         2|\n",
      "|  1|  Alice|        IT| 75000|2022-01-01|   3|         3|         3|\n",
      "|  2|    Bob|        IT| 65000|2021-06-15|   4|         4|         4|\n",
      "+---+-------+----------+------+----------+----+----------+----------+\n",
      "\n",
      "\n",
      "Salaire moyen mobile par département:\n",
      "+---+-------+----------+------+----------+------------------+\n",
      "| id|   name|department|salary| hire_date|running_avg_salary|\n",
      "+---+-------+----------+------+----------+------------------+\n",
      "|  6|  Frank|   Finance| 70000|2023-01-01|           70000.0|\n",
      "|  7|  Grace|   Finance| 80000|2021-12-01|           75000.0|\n",
      "|  3|Charlie|        HR| 55000|2023-03-01|           55000.0|\n",
      "|  5|    Eve|        HR| 60000|2022-07-01|           57500.0|\n",
      "|  2|    Bob|        IT| 65000|2021-06-15|           65000.0|\n",
      "|  1|  Alice|        IT| 75000|2022-01-01|           70000.0|\n",
      "|  4|  David|        IT| 85000|2021-01-01|           75000.0|\n",
      "|  8|  Henry|        IT| 90000|2022-09-15|           78750.0|\n",
      "+---+-------+----------+------+----------+------------------+\n",
      "\n",
      "\n",
      "Comparaison avec salaires précédent et suivant:\n",
      "+---+-------+----------+------+----------+-----------+-----------+--------------+\n",
      "| id|   name|department|salary| hire_date|prev_salary|next_salary|diff_from_prev|\n",
      "+---+-------+----------+------+----------+-----------+-----------+--------------+\n",
      "|  6|  Frank|   Finance| 70000|2023-01-01|       NULL|      80000|          NULL|\n",
      "|  7|  Grace|   Finance| 80000|2021-12-01|      70000|       NULL|         10000|\n",
      "|  3|Charlie|        HR| 55000|2023-03-01|       NULL|      60000|          NULL|\n",
      "|  5|    Eve|        HR| 60000|2022-07-01|      55000|       NULL|          5000|\n",
      "|  2|    Bob|        IT| 65000|2021-06-15|       NULL|      75000|          NULL|\n",
      "|  1|  Alice|        IT| 75000|2022-01-01|      65000|      85000|         10000|\n",
      "|  4|  David|        IT| 85000|2021-01-01|      75000|      90000|         10000|\n",
      "|  8|  Henry|        IT| 90000|2022-09-15|      85000|       NULL|          5000|\n",
      "+---+-------+----------+------+----------+-----------+-----------+--------------+\n",
      "\n",
      "\n",
      "Statistiques cumulatives par département:\n",
      "+---+-------+----------+------+----------+--------------------+----------------------+-----------------+\n",
      "| id|   name|department|salary| hire_date|cumulative_employees|cumulative_salary_cost|avg_salary_so_far|\n",
      "+---+-------+----------+------+----------+--------------------+----------------------+-----------------+\n",
      "|  7|  Grace|   Finance| 80000|2021-12-01|                   1|                 80000|          80000.0|\n",
      "|  6|  Frank|   Finance| 70000|2023-01-01|                   2|                150000|          75000.0|\n",
      "|  5|    Eve|        HR| 60000|2022-07-01|                   1|                 60000|          60000.0|\n",
      "|  3|Charlie|        HR| 55000|2023-03-01|                   2|                115000|          57500.0|\n",
      "|  4|  David|        IT| 85000|2021-01-01|                   1|                 85000|          85000.0|\n",
      "|  2|    Bob|        IT| 65000|2021-06-15|                   2|                150000|          75000.0|\n",
      "|  1|  Alice|        IT| 75000|2022-01-01|                   3|                225000|          75000.0|\n",
      "|  8|  Henry|        IT| 90000|2022-09-15|                   4|                315000|          78750.0|\n",
      "+---+-------+----------+------+----------+--------------------+----------------------+-----------------+\n",
      "\n",
      "\n",
      "Percentiles des salaires par département:\n",
      "+---+-------+----------+------+----------+------------------+\n",
      "| id|   name|department|salary| hire_date|        percentile|\n",
      "+---+-------+----------+------+----------+------------------+\n",
      "|  6|  Frank|   Finance| 70000|2023-01-01|               0.0|\n",
      "|  7|  Grace|   Finance| 80000|2021-12-01|               1.0|\n",
      "|  3|Charlie|        HR| 55000|2023-03-01|               0.0|\n",
      "|  5|    Eve|        HR| 60000|2022-07-01|               1.0|\n",
      "|  2|    Bob|        IT| 65000|2021-06-15|               0.0|\n",
      "|  1|  Alice|        IT| 75000|2022-01-01|0.3333333333333333|\n",
      "|  4|  David|        IT| 85000|2021-01-01|0.6666666666666666|\n",
      "|  8|  Henry|        IT| 90000|2022-09-15|               1.0|\n",
      "+---+-------+----------+------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex10\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, col, avg, sum, percent_rank\n",
    "\n",
    "# Créer des données pour les employés\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"IT\", 75000, \"2022-01-01\"),\n",
    "    (2, \"Bob\", \"IT\", 65000, \"2021-06-15\"),\n",
    "    (3, \"Charlie\", \"HR\", 55000, \"2023-03-01\"),\n",
    "    (4, \"David\", \"IT\", 85000, \"2021-01-01\"),\n",
    "    (5, \"Eve\", \"HR\", 60000, \"2022-07-01\"),\n",
    "    (6, \"Frank\", \"Finance\", 70000, \"2023-01-01\"),\n",
    "    (7, \"Grace\", \"Finance\", 80000, \"2021-12-01\"),\n",
    "    (8, \"Henry\", \"IT\", 90000, \"2022-09-15\")\n",
    "]\n",
    "\n",
    "# Créer le DataFrame\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data, \n",
    "    [\"id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "# 1. Classement des salaires par département\n",
    "print(\"Classement des salaires par département:\")\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "ranked_salaries = employees_df.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "ranked_salaries.show()\n",
    "\n",
    "# 2. Calcul du salaire moyen mobile par département\n",
    "print(\"\\nSalaire moyen mobile par département:\")\n",
    "window_avg = Window.partitionBy(\"department\").orderBy(\"salary\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "avg_salaries = employees_df.withColumn(\n",
    "    \"running_avg_salary\", \n",
    "    avg(\"salary\").over(window_avg)\n",
    ")\n",
    "\n",
    "avg_salaries.show()\n",
    "\n",
    "# 3. Comparaison avec le salaire précédent et suivant\n",
    "print(\"\\nComparaison avec salaires précédent et suivant:\")\n",
    "window_lag_lead = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "comparison = employees_df.withColumn(\n",
    "    \"prev_salary\", \n",
    "    lag(\"salary\").over(window_lag_lead)\n",
    ") \\\n",
    ".withColumn(\n",
    "    \"next_salary\", \n",
    "    lead(\"salary\").over(window_lag_lead)\n",
    ") \\\n",
    ".withColumn(\n",
    "    \"diff_from_prev\", \n",
    "    col(\"salary\") - col(\"prev_salary\")\n",
    ")\n",
    "\n",
    "comparison.show()\n",
    "\n",
    "# 4. Statistiques cumulatives\n",
    "print(\"\\nStatistiques cumulatives par département:\")\n",
    "window_cumulative = Window.partitionBy(\"department\").orderBy(\"hire_date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "cumulative_stats = employees_df.withColumn(\n",
    "    \"cumulative_employees\", \n",
    "    count(\"*\").over(window_cumulative)\n",
    ") \\\n",
    ".withColumn(\n",
    "    \"cumulative_salary_cost\", \n",
    "    sum(\"salary\").over(window_cumulative)\n",
    ") \\\n",
    ".withColumn(\n",
    "    \"avg_salary_so_far\", \n",
    "    avg(\"salary\").over(window_cumulative)\n",
    ")\n",
    "\n",
    "cumulative_stats.orderBy(\"department\", \"hire_date\").show()\n",
    "\n",
    "# 5. Calcul des percentiles par département\n",
    "print(\"\\nPercentiles des salaires par département:\")\n",
    "window_ntile = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "percentiles = employees_df.withColumn(\n",
    "    \"percentile\", \n",
    "    percent_rank().over(window_ntile)\n",
    ")\n",
    "\n",
    "percentiles.orderBy(\"department\", \"salary\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
