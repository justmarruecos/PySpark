{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Première ligne du fichier worldcitiespop.txt :\n",
      "Country,City,AccentCity,Region,Population,Latitude,Longitude\n"
     ]
    }
   ],
   "source": [
    "#ex1\n",
    "# \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création de la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lecture WorldCitiesPop\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lire le fichier worldcitiespop.txt\n",
    "cities_rdd = spark.sparkContext.textFile(\"C:\\EXERCICES\\pyspark\\PySpark\\worldcitiespop.txt\")\n",
    "\n",
    "# Afficher la première ligne avec first()\n",
    "print(\"Première ligne du fichier worldcitiespop.txt :\")\n",
    "print(cities_rdd.first())\n",
    "\n",
    "# Arrêter la session Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques du nettoyage :\n",
      "------------------------------\n",
      "Nombre total de lignes (sans en-tête): 3173958\n",
      "Nombre de lignes avec population valide: 47980\n",
      "Pourcentage de lignes valides: 1.51%\n",
      "\n",
      "Exemples de lignes valides:\n",
      "------------------------------\n",
      "ad,andorra la vella,Andorra la Vella,07,20430,42.5,1.5166667\n",
      "ad,canillo,Canillo,02,3292,42.5666667,1.6\n",
      "ad,encamp,Encamp,03,11224,42.5333333,1.5833333\n",
      "ad,la massana,La Massana,04,7211,42.55,1.5166667\n",
      "ad,les escaldes,Les Escaldes,08,15854,42.5,1.5333333\n"
     ]
    }
   ],
   "source": [
    "#ex1 première méthode\n",
    "# \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Création de la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Nettoyage WorldCitiesPop\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le chemin du fichier\n",
    "file_path = \"C:/EXERCICES/pyspark/PySpark/worldcitiespop.txt\"\n",
    "\n",
    "# Lire le fichier\n",
    "cities_rdd = spark.sparkContext.textFile(file_path)\n",
    "\n",
    "# Fonction pour parser une ligne et vérifier si la population est valide\n",
    "def parse_line(line):\n",
    "    try:\n",
    "        fields = line.split(',')\n",
    "        if len(fields) >= 5:  # Vérifier qu'il y a au moins 5 champs\n",
    "            population = fields[4]\n",
    "            # Vérifier si la population est un nombre non vide et positif\n",
    "            if population and float(population) > 0:\n",
    "                return True\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Filtrer les lignes avec population valide\n",
    "# Ignorer la première ligne (en-têtes)\n",
    "header = cities_rdd.first()\n",
    "valid_cities = cities_rdd.filter(lambda x: x != header).filter(parse_line)\n",
    "\n",
    "# Afficher quelques statistiques\n",
    "total_lines = cities_rdd.count() - 1  # -1 pour l'en-tête\n",
    "valid_lines = valid_cities.count()\n",
    "\n",
    "print(\"Statistiques du nettoyage :\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nombre total de lignes (sans en-tête): {total_lines}\")\n",
    "print(f\"Nombre de lignes avec population valide: {valid_lines}\")\n",
    "print(f\"Pourcentage de lignes valides: {(valid_lines/total_lines)*100:.2f}%\")\n",
    "\n",
    "# Afficher quelques exemples de lignes valides\n",
    "print(\"\\nExemples de lignes valides:\")\n",
    "print(\"-\" * 30)\n",
    "for line in valid_cities.take(5):\n",
    "    print(line)\n",
    "\n",
    "# Sauvegarder les résultats si nécessaire\n",
    "# valid_cities.saveAsTextFile(\"C:/EXERCICES/pyspark/PySpark/valid_cities\")\n",
    "\n",
    "# Arrêter la session Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques du nettoyage :\n",
      "------------------------------\n",
      "Nombre total de lignes (sans en-tête): 3173958\n",
      "Nombre de lignes avec population valide: 47980\n",
      "Pourcentage de lignes valides: 1.51%\n",
      "\n",
      "Exemples de lignes valides:\n",
      "------------------------------\n",
      "ad,andorra la vella,Andorra la Vella,07,20430,42.5,1.5166667\n",
      "ad,canillo,Canillo,02,3292,42.5666667,1.6\n",
      "ad,encamp,Encamp,03,11224,42.5333333,1.5833333\n",
      "ad,la massana,La Massana,04,7211,42.55,1.5166667\n",
      "ad,les escaldes,Les Escaldes,08,15854,42.5,1.5333333\n"
     ]
    }
   ],
   "source": [
    "#ex2 avec math\n",
    "\n",
    "from pyspark import *\n",
    "from math import *\n",
    "\n",
    "# Création du SparkContext\n",
    "sc = SparkContext(\"local\", \"Exercice 2\")\n",
    "\n",
    "# Définir le chemin du fichier\n",
    "file_path = \"C:/EXERCICES/pyspark/PySpark/worldcitiespop.txt\"\n",
    "\n",
    "# Lire le fichier\n",
    "cities_rdd = sc.textFile(file_path)\n",
    "\n",
    "# Fonction pour parser une ligne et vérifier si la population est valide\n",
    "def parse_line(line):\n",
    "    try:\n",
    "        fields = line.split(',')\n",
    "        if len(fields) >= 5:  # Vérifier qu'il y a au moins 5 champs\n",
    "            population = fields[4]\n",
    "            # Vérifier si la population est un nombre non vide et positif\n",
    "            if population and float(population) > 0:\n",
    "                return True\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Filtrer les lignes avec population valide\n",
    "# Ignorer la première ligne (en-têtes)\n",
    "header = cities_rdd.first()\n",
    "valid_cities = cities_rdd.filter(lambda x: x != header).filter(parse_line)\n",
    "\n",
    "# Afficher quelques statistiques\n",
    "total_lines = cities_rdd.count() - 1  # -1 pour l'en-tête\n",
    "valid_lines = valid_cities.count()\n",
    "\n",
    "print(\"Statistiques du nettoyage :\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Nombre total de lignes (sans en-tête): {total_lines}\")\n",
    "print(f\"Nombre de lignes avec population valide: {valid_lines}\")\n",
    "print(f\"Pourcentage de lignes valides: {(valid_lines/total_lines)*100:.2f}%\")\n",
    "\n",
    "# Afficher quelques exemples de lignes valides\n",
    "print(\"\\nExemples de lignes valides:\")\n",
    "print(\"-\" * 30)\n",
    "for line in valid_cities.take(5):\n",
    "    print(line)\n",
    "\n",
    "# Arrêter le SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques sur les populations:\n",
      "count: 47980\n",
      "mean: 47719.57063359733\n",
      "min: 7.0\n",
      "max: 31480498.0\n",
      "\n",
      "Histogramme des populations (échelle logarithmique):\n",
      "Classe | Nombre de villes\n",
      "------------------------------\n",
      "10^0 | 5\n",
      "10^1 | 174\n",
      "10^2 | 2187\n",
      "10^3 | 20537\n",
      "10^4 | 21550\n",
      "10^5 | 3248\n",
      "10^6 | 269\n",
      "10^7 | 10\n"
     ]
    }
   ],
   "source": [
    "#ex 3 et ex 4\n",
    "\n",
    "from pyspark import *\n",
    "from math import *\n",
    "\n",
    "# Création du SparkContext\n",
    "sc = SparkContext(\"local\", \"Exercices 3 et 4\")\n",
    "\n",
    "# Lire le fichier\n",
    "cities_rdd = sc.textFile(\"C:/EXERCICES/pyspark/PySpark/worldcitiespop.txt\")\n",
    "\n",
    "# Fonction pour extraire la population si valide\n",
    "def get_population(line):\n",
    "    try:\n",
    "        fields = line.split(',')\n",
    "        if len(fields) >= 5:\n",
    "            population = fields[4]\n",
    "            if population and float(population) > 0:\n",
    "                return float(population)\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Ignorer l'en-tête et extraire les populations valides\n",
    "header = cities_rdd.first()\n",
    "populations = cities_rdd.filter(lambda x: x != header) \\\n",
    "                      .map(get_population) \\\n",
    "                      .filter(lambda x: x is not None)\n",
    "\n",
    "### Exercice 3 : Statistiques ###\n",
    "# Calculer les statistiques\n",
    "count = populations.count()\n",
    "sum_pop = populations.sum()\n",
    "min_pop = populations.min()\n",
    "max_pop = populations.max()\n",
    "avg_pop = sum_pop / count\n",
    "\n",
    "print(\"Statistiques sur les populations:\")\n",
    "print(f\"count: {count}\")\n",
    "print(f\"mean: {avg_pop}\")\n",
    "print(f\"min: {min_pop}\")\n",
    "print(f\"max: {max_pop}\")\n",
    "\n",
    "### Exercice 4 : Histogrammes ###\n",
    "# Fonction pour déterminer la classe (en échelle logarithmique)\n",
    "def get_class(population):\n",
    "    if population > 0:\n",
    "        return floor(log10(population))\n",
    "    return 0\n",
    "\n",
    "# Calculer l'histogramme\n",
    "histogram = populations.map(lambda x: (get_class(x), 1)) \\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .sortByKey() \\\n",
    "                      .collect()\n",
    "\n",
    "print(\"\\nHistogramme des populations (échelle logarithmique):\")\n",
    "print(\"Classe | Nombre de villes\")\n",
    "print(\"-\" * 30)\n",
    "for classe, count in histogram:\n",
    "    print(f\"10^{classe} | {count}\")\n",
    "\n",
    "# Arrêter le SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 10 villes les plus peuplées:\n",
      "--------------------------------------------------\n",
      "Pays | Ville | Ville (avec accents) | Région | Population\n",
      "--------------------------------------------------\n",
      "jp, tokyo, Tokyo, 40, 31480498\n",
      "cn, shanghai, Shanghai, 23, 14608512\n",
      "in, bombay, Bombay, 16, 12692717\n",
      "pk, karachi, Karachi, 05, 11627378\n",
      "in, delhi, Delhi, 07, 10928270\n",
      "in, new delhi, New Delhi, 07, 10928270\n",
      "ph, manila, Manila, D9, 10443877\n",
      "ru, moscow, Moscow, 48, 10381288\n",
      "kr, seoul, Seoul, 11, 10323448\n",
      "br, sao paulo, S�o Paulo, 27, 10021437\n"
     ]
    }
   ],
   "source": [
    "#ex5\n",
    "\n",
    "from pyspark import *\n",
    "from math import *\n",
    "\n",
    "# Création du SparkContext\n",
    "sc = SparkContext(\"local\", \"Exercice 5\")\n",
    "\n",
    "# Lire le fichier\n",
    "cities_rdd = sc.textFile(\"C:/EXERCICES/pyspark/PySpark/worldcitiespop.txt\")\n",
    "\n",
    "# Fonction pour extraire les informations de ville avec population\n",
    "def parse_city(line):\n",
    "    try:\n",
    "        fields = line.split(',')\n",
    "        if len(fields) >= 6:  # Au moins 6 champs nécessaires\n",
    "            country = fields[0]\n",
    "            city = fields[1]\n",
    "            accent_city = fields[2]\n",
    "            region = fields[3]\n",
    "            population = fields[4]\n",
    "            if population and float(population) > 0:\n",
    "                return (country, city, accent_city, region, float(population))\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Ignorer l'en-tête et extraire les villes avec population valide\n",
    "header = cities_rdd.first()\n",
    "valid_cities = cities_rdd.filter(lambda x: x != header) \\\n",
    "                        .map(parse_city) \\\n",
    "                        .filter(lambda x: x is not None)\n",
    "\n",
    "# Trier par population décroissante et prendre les 10 premières villes\n",
    "top_10_cities = valid_cities.sortBy(lambda x: -x[4]) \\\n",
    "                           .take(10)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Les 10 villes les plus peuplées:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Pays | Ville | Ville (avec accents) | Région | Population\")\n",
    "print(\"-\" * 50)\n",
    "for city in top_10_cities:\n",
    "    print(f\"{city[0]}, {city[1]}, {city[2]}, {city[3]}, {int(city[4])}\")\n",
    "\n",
    "# Arrêter le SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 37575, mean: 57219.79, stdev: 340982.15, max: 31480498.0, min: 7.0)\n",
      "[(0, 4), (1, 61), (2, 1346), (3, 14449), (4, 18362), (5, 3078), (6, 265), (7, 10)]\n",
      "jp,tokyo,Tokyo,40,31480498,35.685,139.751389\n",
      "cn,shanghai,Shanghai,23,14608512,31.045556,121.399722\n",
      "in,bombay,Bombay,16,12692717,18.975,72.825833\n",
      "pk,karachi,Karachi,05,11627378,24.9056,67.0822\n",
      "in,delhi,Delhi,07,10928270,28.666667,77.216667\n",
      "in,new delhi,New Delhi,07,10928270,28.6,77.2\n",
      "ph,manila,Manila,D9,10443877,14.6042,120.9822\n",
      "ru,moscow,Moscow,48,10381288,55.752222,37.615556\n",
      "kr,seoul,Seoul,11,10323448,37.5985,126.9783\n",
      "br,sao paulo,S�o Paulo,27,10021437,-23.473293,-46.665803\n",
      "tr,istanbul,Istanbul,34,9797536,41.018611,28.964722\n",
      "ng,lagos,Lagos,05,8789133,6.453056,3.395833\n",
      "mx,mexico,Mexico,09,8720916,19.434167,-99.138611\n",
      "id,jakarta,Jakarta,04,8540306,-6.174444,106.829444\n",
      "us,new york,New York,NY,8107916,40.7141667,-74.0063889\n",
      "cd,kinshasa,Kinshasa,06,7787832,-4.3,15.3\n",
      "eg,cairo,Cairo,11,7734602,30.05,31.25\n",
      "pe,lima,Lima,15,7646786,-12.05,-77.05\n",
      "cn,peking,Peking,22,7480601,39.928889,116.388333\n",
      "gb,london,London,H9,7421228,51.514125,-0.093689\n"
     ]
    }
   ],
   "source": [
    "#ex 6\n",
    "\n",
    "from pyspark import *\n",
    "from math import *\n",
    "\n",
    "# Création du SparkContext\n",
    "sc = SparkContext(\"local\", \"Exercice 6 - Re-cleaning\")\n",
    "\n",
    "# Lire le fichier\n",
    "cities_rdd = sc.textFile(\"C:/EXERCICES/pyspark/PySpark/worldcitiespop.txt\")\n",
    "\n",
    "# Fonction pour parser les villes avec leurs coordonnées\n",
    "def parse_city_with_coords(line):\n",
    "    try:\n",
    "        fields = line.split(',')\n",
    "        if len(fields) >= 7 and fields[4]:  # Vérifier population et coordonnées\n",
    "            country = fields[0]\n",
    "            city = fields[1]\n",
    "            accent_city = fields[2]\n",
    "            region = fields[3]\n",
    "            population = float(fields[4])\n",
    "            latitude = float(fields[5])\n",
    "            longitude = float(fields[6])\n",
    "            if population > 0:\n",
    "                # Clé: coordonnées arrondies pour regrouper les villes proches\n",
    "                key = (round(latitude, 1), round(longitude, 1))\n",
    "                # Valeur: toutes les informations de la ville\n",
    "                value = (country, city, accent_city, region, population, latitude, longitude)\n",
    "                return (key, value)\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Traitement des données\n",
    "header = cities_rdd.first()\n",
    "cleaned_cities = cities_rdd.filter(lambda x: x != header) \\\n",
    "    .map(parse_city_with_coords) \\\n",
    "    .filter(lambda x: x is not None) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda cities: max(cities, key=lambda x: x[4])) \\\n",
    "    .values()\n",
    "\n",
    "# Calculer les statistiques\n",
    "populations = cleaned_cities.map(lambda x: x[4])\n",
    "count = populations.count()\n",
    "mean = populations.mean()\n",
    "stdev = populations.stdev()\n",
    "max_pop = populations.max()\n",
    "min_pop = populations.min()\n",
    "\n",
    "print(f\"(count: {count}, mean: {mean:.2f}, stdev: {stdev:.2f}, max: {max_pop}, min: {min_pop})\")\n",
    "\n",
    "# Calculer l'histogramme\n",
    "def get_class(population):\n",
    "    return floor(log10(population))\n",
    "\n",
    "histogram = populations.map(lambda x: (get_class(x), 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortByKey() \\\n",
    "    .collect()\n",
    "\n",
    "print(histogram)\n",
    "\n",
    "# Afficher les 20 plus grandes villes\n",
    "top_cities = cleaned_cities.sortBy(lambda x: -x[4]).take(20)\n",
    "\n",
    "# Formater et afficher les résultats\n",
    "for city in top_cities:\n",
    "    country, city_name, accent_city, region, pop, lat, lon = city\n",
    "    print(f\"{country},{city_name},{accent_city},{region},{int(pop)},{lat},{lon}\")\n",
    "\n",
    "# Arrêter le SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
